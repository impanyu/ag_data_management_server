{% extends 'layouts/base.html' %}

{% block title %} Tables {% endblock title %}

{% block content %}

<div class ="bg-unl" >
 <div class="card" style = "border-radius:0px;padding:30px">
   <header>
  <h1>Documentation</h1>
   </header>
  <h2>Content</h2>
  <nav>
    <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#architecture">Key Features</a></li>
        <li><a href="#search_module">Applications</a></li>
        <li><a href="#file">Contact Us</a></li>
       <li><a href="#tools_and_models">Introduction</a></li>
        <li><a href="#collections">Key Features</a></li>
        <li><a href="#privacy">Applications</a></li>
        <li><a href="#jupyter">Contact Us</a></li>
    </ul>
</nav>
  <main>
   <section id="introduction">
  <h2>Introduction</h2>
  <p style="font-weight:400"> Agriculture Data Management and Analytics(ADMA) is a comprehensive data management platform developed for experts in and outside IANR. It is a state-of-the-art solution designed to address the multifaceted challenges faced by modern agriculture. As the world grapples with population growth, climate change, and diminishing natural resources, the need for data-driven innovation in agroecosystems has never been more critical.

ADMA is a comprehensive platform that harnesses the power of advanced technologies, such as IoT, high-resolution sensors, and cloud computing, to facilitate data collection, integration, and analysis across various agricultural disciplines. By incorporating the FAIR principles (Findable, Accessible, Interoperable, and Reusable), ADMA aims to break down data silos and promote collaboration among researchers, policymakers, and industry stakeholders.

Key features of the ADMA platform include:

  <ul>
  <li>
   <b>Interactive Data Management and Analysis Portals</b>: Accessible through web GUI, command line, and API, these user-friendly interfaces empower users to manage, analyze, and share data with ease.
  </li>
  <li>
 <b>Scalability and Flexibility</b>: Leveraging the capabilities of High-Performance Computing (HPC), ADMA can efficiently process large volumes of heterogeneous data, while also allowing users to incorporate their own data management and analysis tools.
  </li>
  <li>
<b>Open-source Technology</b>: Built upon a robust framework of open-source technologies, ADMA fosters a collaborative environment for continuous improvement and innovation.
</li>
  <li>
   <b>Pipeline management</b>: ADMA keeps track of different operations on each file, maintains and visualize the pipeline record for each file.
  </li>
 </ul>
By providing an integrated platform for agricultural data management and analysis, ADMA is poised to play a pivotal role in enhancing the productivity, sustainability, and resilience of our agroecosystems. Join us in revolutionizing the future of agriculture through data-driven insights and informed decision-making.

  </p>
   </section>
  <section id="architecture" class="row align-items-center ">
  <h2 class="col-lg-12 col-12 ">System Architecture</h2>
      <div class="col-lg-2 col-12"></div>
      <div class="col-lg-8 col-12 text-center">
          <figure>
          <img src="/static/assets/img/agricultural_data_management_platform.png" style="width:100%" >
               <figcaption><b>Figure 1. System Architecture</b></figcaption>
          </figure>
      </div>
      <p style="font-weight:400">
          Figure 1 illustrates the overall architecture of the systems. There are <b>six components</b>: data source, batch processing, real-time processing, actuators, server and Front end.
<ul>
      <li>
          The data source is mainly composed of four parts: RDBMS, NoSQL database and file system for storing the historical data, sensors for collecting real-time data streams, metadata for storing the formats and configuration about all the data and tools.
</li>
<li>
Batch processing component is the engine of the whole system, where big data processing occurs. This component can be further divided into two environments: container environment and native environment. The container environment can host various types of containerized tools such as R, Python, Matlab, Tensorflow, Cuda, MPI and many more customized tools for the specific disciplines. There is a  container scheduler such as swarm or Kubernetes to manage the running of containers. Various user defined data processing and model training can be conducted in the container environment. The native environment contains all the necessary parts in a typical big data configuration, such as HDFS for file management, YARN for scheduling, SPARK for data processing, HIVE for data retrieving and HBase for data storage. We decided to use map reduce based data processing paradigm to accommodate various types of data processing requirements.  The output of the batch processing will be stored back to the data source for further retrieval and processing.
</li>
      <li>
Real-time processing component connects to the sensors and uses pipes to stream the real-time data for further processing. Several open source frameworks can be used here: Kafka manages a pipe for data stream collection, Spark Streaming/Flink/Storm are options for real-time data stream processing and Redis is an in-memory database for low latency data storage. Container environment is also supported, where hosted pre-trained models and customized tools can process data and make decisions in real-time or near real-time fashion.  The real time analytical results can be fed to the data processing module and data service API inside the server component. The decisions and commands can be output to the actuator to execute. This component is a necessary complement to batch processing when real-time analysis and control is required and relatively low accuracy is tolerated.
      </li>
      <li>
The actuator is included in the system to make it more like a fully functioning control system. Connected to the real-time processing component, the actuator executes the commands/decisions output by the hosted models or user coded tools. The types of actuators vary among different disciplines. For biological system engineering, to name a few, there are actuators such as irrigation systems, robotics and field operation machines.
      </li>
      <li>
The server is the operational handler of the whole system. Data management module is responsible for the ordinary operation such as file search, retrieve, upload, delete, update and processing. Version control is also considered in the future implementation. Tool management module keeps the information of the tool sets used in the system. Model management module is responsible for maintaining all the pre-trained models. Data processing module talks to and manages the batch processing and real-time processing components. This module is an umbrella module for all the data processing jobs such as data pre-processing, data co-locating, batch processing, model training,  model hosting, real-time processing, etc.  Security and authentication module guarantees the data storage & transmission is secure and each user has a separate access permission and view to the data. Data service API acts as the portal for the front-end to manipulate the data in our system.
      </li>
      <li>
The front end runs on the user side machine and can take the forms of web browser, command-line or third-party software. The front end communicates with the server through the data service API. Users of our system can conduct ordinary file management operations and initiate data processing requests to the system. The analytical results can be displayed in the front end for the user to view.
      </li>
  </ul>

      </p>
  </section>
   <section id="search_module">
  <h2> Search </h2>
   </section>
   <section id="file">
  <h2> File Management</h2>
   </section>
   <section id="tools_and_models">
  <h2> Tools and Models</h2>
   </section>
   <section id="collections">
  <h2> Collections</h2>
   </section>
   <section id="privacy">
  <h2> Data Privacy</h2>
   </section>
   <section id="jupyter">
  <h2> JupyterHub</h2>
   </section>
</main>

  </div>


</div>


{% include "includes/footer.html" %}

{% endblock content %}

<!-- Specific JS goes HERE -->
{% block javascripts %}



<script src="https://d3js.org/d3.v6.min.js"></script>



{% endblock javascripts %}
